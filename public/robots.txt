# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
#
# To allow all crawlers to access all parts of the site, use:
User-agent: *
Allow: /

# To disallow all crawlers from accessing the site, use:
# User-agent: *
# Disallow: /

# Point crawlers to the sitemap
Sitemap: /sitemap.xml
